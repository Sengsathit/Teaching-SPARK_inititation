{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54f63150",
   "metadata": {},
   "source": [
    "![SPARK_PYSPARK](img/spark_pyspark.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a877b0ab",
   "metadata": {},
   "source": [
    "# üí´ Apache Spark : le moteur du Big Data moderne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4419d5",
   "metadata": {},
   "source": [
    "Le **Big Data** d√©signe des volumes de donn√©es trop grands pour √™tre trait√©s sur une seule machine.\n",
    "Historiquement, on utilisait **Hadoop MapReduce**, mais celui-ci lisait et √©crivait souvent sur disque ‚Üí lent.\n",
    "\n",
    "**Apache Spark** est n√© pour r√©pondre √† ce probl√®me.\n",
    "- **Traitement en m√©moire (RAM)** ‚Üí bien plus rapide\n",
    "- **API haut niveau** (SQL, DataFrame, MLlib, etc.)\n",
    "- **Multi-langages** : Scala, Java, Python (PySpark), R\n",
    "\n",
    "Spark repose sur une architecture driver / executors.\n",
    "- **Driver** : programme principal. G√®re le plan d‚Äôex√©cution et collecte les r√©sultats\n",
    "- **Cluster Manager** : alloue les ressources (CPU, RAM) aux t√¢ches Spark (YARN, Kubernetes, Standalone‚Ä¶)\n",
    "- **Executors** : ex√©cutent les t√¢ches sur les partitions de donn√©es.\n",
    "\n",
    "Spark distribue automatiquement les donn√©es et le calcul sur plusieurs machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a2174d",
   "metadata": {},
   "source": [
    "# ‚ú® PySpark : l‚Äôinterface Python de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93533013",
   "metadata": {},
   "source": [
    "**PySpark** est l‚Äôinterface de programmation en Python pour Apache Spark.\\\n",
    "Elle permet d‚Äôutiliser la puissance du moteur Spark tout en restant dans un environnement familier pour les d√©veloppeurs Python.\n",
    "\n",
    "Concr√®tement :\n",
    "- Le code √©crit en Python est traduit en instructions que le moteur Spark ex√©cute sur le cluster.\n",
    "- L‚Äôex√©cution r√©elle ne se fait pas dans le processus Python, mais dans la JVM de Spark (Java Virtual Machine).\n",
    "- Cette communication passe par l‚ÄôAPI Py4J, une passerelle entre Python et Java.\n",
    "\n",
    "Ainsi, PySpark combine :\n",
    "- la simplicit√© du Python (syntaxe, int√©gration avec pandas, notebooks, etc.)\n",
    "- la scalabilit√© de Spark, capable de traiter des t√©raoctets de donn√©es sur des centaines de noeuds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfea9e4",
   "metadata": {},
   "source": [
    "# ‚å®Ô∏è Hands-On"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd3bff",
   "metadata": {},
   "source": [
    "**Objectifs**  \n",
    "- Comprendre comment d√©marrer une `SparkSession` en local  \n",
    "- Manipuler des DataFrames PySpark (lecture, inspection, transformations)  \n",
    "- Comprendre `lazy evaluation`, `action`, `job / stage / task`  \n",
    "- Sauvegarder/relire des donn√©es (CSV, Parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615a17a9",
   "metadata": {},
   "source": [
    "## 1. D√©marrer Spark en local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa260e35",
   "metadata": {},
   "source": [
    "Avant de pouvoir manipuler des donn√©es avec PySpark, il faut cr√©er une session Spark.\\\n",
    "Cette session est le point d‚Äôentr√©e principal vers toutes les fonctionnalit√©s de Spark (lecture de donn√©es, SQL, transformations, machine learning, etc.).\n",
    "\n",
    "En pratique, on la cr√©e √† l‚Äôaide de la classe **SparkSession** qui permet de configurer et d‚Äôinitialiser le moteur Spark.\\\n",
    "Une fois cette cellule ex√©cut√©e, Spark d√©marre un contexte local et affiche des informations dans la console.\n",
    "C‚Äôest le signe que le driver est pr√™t √† ex√©cuter des t√¢ches et √† communiquer avec d‚Äô√©ventuels executors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7036a41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IntroSparkLocal\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# SparkSession.builder cr√©e un objet constructeur pour d√©finir les param√®tres de la session.\n",
    "# .appName(\"IntroSparkLocal\") d√©finit un nom d‚Äôapplication visible dans l‚Äôinterface Spark (utile pour identifier ton job).\n",
    "# .master(\"local[*]\") indique le mode d‚Äôex√©cution :\n",
    "#       - local signifie que le calcul se fera sur la machine locale.\n",
    "#       - [*] indique que Spark utilisera tous les c≈ìurs CPU disponibles.\n",
    "# .getOrCreate() cr√©e une nouvelle session Spark si elle n‚Äôexiste pas encore, ou r√©cup√®re celle d√©j√† active."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac97a8ca",
   "metadata": {},
   "source": [
    "Aussi, Spark d√©marre une interface web accessible √† l‚Äôadresse suivante : http://localhost:4040\n",
    "\n",
    "Cette interface permet de :\n",
    "- visualiser les jobs Spark en cours ou termin√©s\n",
    "- consulter les stages et tasks\n",
    "- analyser les plans d‚Äôex√©cution\n",
    "- suivre l‚Äôutilisation des ressources locales (m√©moire, CPU)\n",
    "\n",
    "### üß≠ Exploration recommand√©e\n",
    "\n",
    "Ouvrir l‚ÄôURL http://localhost:4040 dans le navigateur.\n",
    "Parcourir les diff√©rentes sections pour comprendre comment Spark g√®re un traitement distribu√© :\n",
    "- Jobs : r√©sum√© des traitements lanc√©s,\n",
    "- Stages : d√©composition logique des jobs,\n",
    "- Tasks : ex√©cution concr√®te sur les partitions de donn√©es,\n",
    "- Storage : aper√ßu du cache et des donn√©es en m√©moire.\n",
    "\n",
    "Prendre quelques minutes pour parcourir cette interface avant de continuer car elle aidera √† visualiser ce qui se passe ‚Äúsous le capot‚Äù √† chaque fois qu'une op√©ration est ex√©cut√©e dans PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26041953",
   "metadata": {},
   "source": [
    "## 2. Charger le dataset de test :  `employees.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cba23d9",
   "metadata": {},
   "source": [
    "Pour travailler concr√®tement avec Spark, nous allons charger un petit dataset de test nomm√© `employees.csv`.\\\n",
    "Ce fichier contient des informations simples sur des employ√©s : identifiant, nom, √¢ge, d√©partement, salaire et ann√©es d‚Äôexp√©rience.\n",
    "\n",
    "Dans PySpark, la lecture d‚Äôun fichier CSV s‚Äôeffectue via la m√©thode `spark.read.csv()`.\n",
    "Cette m√©thode renvoie un **DataFrame Spark,** c‚Äôest-√†-dire une **structure tabulaire distribu√©e** (semblable √† un tableau SQL ou √† un DataFrame Pandas, mais r√©partie sur plusieurs noeuds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c346cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charge le fichier csv\n",
    "# header=True indique que la premi√®re ligne contient les noms des colonnes\n",
    "# inferSchema=True demande √† Spark de d√©duire automatiquement les types de donn√©es sinon toutes les colonnes seraient lues comme des cha√Ænes de caract√®res\n",
    "df = spark.read.csv(\"./data/employees.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Affiche la structure (sch√©ma) du DataFrame, c‚Äôest-√†-dire les noms de colonnes et leurs types \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c9b474",
   "metadata": {},
   "source": [
    "üîç **D√©taillons ce qu‚Äôil se passe en arri√®re-plan.**\n",
    "\n",
    "Lors de la lecture du CSV :\n",
    "1. Le driver Spark re√ßoit la requ√™te de lecture du fichier csv. Il construit un plan logique pour lire le fichier CSV. Le plan logique est une repr√©sentation interne (sous forme d‚Äôun graphe `DAG`) qui d√©crit les √©tapes n√©cessaires pour obtenir le r√©sultat final : *\"quel fichier lire\"*, *\"avec quel format (ici CSV)\"*, *\"quelles colonnes ou types de donn√©es\"*, *\"quelles transformations appliquer (filtres, jointures, agr√©gations, etc.)\"*\\\n",
    "Mais √† ce stade, **rien n‚Äôest encore ex√©cut√©**. Spark se contente de noter l‚Äô**intention du programme**.\n",
    "2. Quand une action est d√©clench√©e (.show(), .count(), etc.), le driver envoie les t√¢ches aux executors.\n",
    "3. Les executors lisent r√©ellement les donn√©es (leurs partitions), ex√©cutent les transformations, et renvoient leurs r√©sultats partiels au driver. (En mode local, les executors sont simplement des threads dans le m√™me processus JVM que le driver.)\n",
    "4. Le driver agr√®ge ces r√©sultats et les renvoie au programme Python.\n",
    "\n",
    "M√™me pour un petit CSV local, le m√©canisme est le m√™me que sur un cluster : Spark applique toujours un mod√®le distribu√©. Cela rend le passage √† grande √©chelle quasiment transparent pour le code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8143eb8c",
   "metadata": {},
   "source": [
    "### 2.1. Inspection de base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bbd41e",
   "metadata": {},
   "source": [
    "Avant de transformer les donn√©es, il est souvent utile de v√©rifier la structure du DataFrame.\\\n",
    "L'op√©ration est imm√©diate : elle interroge seulement les m√©tadonn√©es du DataFrame (pas les donn√©es elles-m√™mes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f14370",
   "metadata": {},
   "source": [
    "### 2.2. Lazy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b274929",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Contrairement √† df.columns, les instructions suivantes d√©clenchent des actions Spark.\\\n",
    "Le moteur Spark ex√©cute r√©ellement des jobs distribu√©s pour lire les donn√©es, parcourir toutes les partitions et agr√©ger les r√©sultats partiels.\n",
    "\n",
    "üîç **D√©taillons ce qu‚Äôil se passe en arri√®re-plan.**\n",
    "- `df.count()` parcourt l‚Äôensemble du DataFrame pour compter le nombre de lignes. Pour cela : \\\n",
    "Spark cr√©e un job compos√© de plusieurs stages et tasks, chacun ex√©cut√© par les executors sur une partition de donn√©es.\\\n",
    "Une fois termin√©es, les executors envoient leurs r√©sultats partiels (leurs counts locaux) au driver.\\\n",
    "Le driver additionne ces valeurs pour obtenir le compte global du DataFrame,\n",
    "puis renvoie le r√©sultat final au programme Python.\n",
    "- `df.show(5)` demande √† Spark d‚Äôafficher les 5 premi√®res lignes du DataFrame.\n",
    "Cela n√©cessite de lire physiquement les donn√©es (au moins partiellement) ‚Üí Spark d√©clenche donc aussi un job distribu√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1b68bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.count())\n",
    "display(df.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefe8c5b",
   "metadata": {},
   "source": [
    "## 3. S√©lections, filtres, tris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa72d7d2",
   "metadata": {},
   "source": [
    "Dans Spark, les DataFrames offrent une interface haut niveau pour manipuler les donn√©es de mani√®re d√©clarative, un peu comme en SQL.\n",
    "Les op√©rations de s√©lection, filtrage et tri sont des transformations qui **ne modifient pas les donn√©es originales** : elles produisent de **nouveaux DataFrames**.\n",
    "\n",
    "üí° Chaque transformation cr√©e un nouveau plan logique dans Spark (**aucune lecture ou ex√©cution r√©elle n‚Äôa lieu** tant qu‚Äôune action, comme .show() ou .count(), n‚Äôest pas appel√©e).\\\n",
    "Ainsi, m√™me un simple tri ou filtre suit la m√™me logique de planification distribu√©e que sur un cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c940c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# S√©lection de colonnes\n",
    "df_sel = df.select(\"name\", \"age\", \"department\")\n",
    "df_sel.show(5)\n",
    "\n",
    "# Filtres\n",
    "employees_filtered = df_sel.filter(col(\"age\") >= 30) # Ne conserver que les employ√©s d'un certain √¢ge\n",
    "employees_filtered = employees_filtered.orderBy(col(\"age\").desc())\n",
    "employees_filtered.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44eb34f",
   "metadata": {},
   "source": [
    "### 3.1. Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f5d1ad",
   "metadata": {},
   "source": [
    "Il est possible d‚Äô√©crire directement du ***SQL*** gr√¢ce √† ***Spark SQL***, un module int√©gr√© √† Apache Spark qui permet d‚Äôinterroger les donn√©es via des requ√™tes SQL standard.\\\n",
    "Pour cela, il suffit d‚Äôenregistrer un DataFrame comme vue temporaire √† l‚Äôaide de `createOrReplaceTempView()`. Cette vue agit comme une table virtuelle accessible uniquement durant la session en cours.\\\n",
    "On peut ensuite ex√©cuter des requ√™tes SQL sur cette vue avec la m√©thode `spark.sql()`, comme dans l‚Äôexemple ci-dessous, qui fait exactement la m√™me chose que le job pr√©c√©dent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5167daaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation ou remplacement d'une vue temporaire √† partir du DataFrame\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "# Requ√™te SQL ex√©cut√©e via Spark SQL\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT department,\n",
    "           AVG(salary) AS avg_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "\"\"\")\n",
    "\n",
    "# Affichage du r√©sultat\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1282cc7",
   "metadata": {},
   "source": [
    "### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0c7eb1",
   "metadata": {},
   "source": [
    "> Essayez de modifier et d‚Äôexp√©rimenter :\n",
    "> - s√©lectionnez d‚Äôautres colonnes avec .select().\n",
    "> - changez la condition du filtre (age < 25, department == \"HR\", etc.).\n",
    "> - triez par une autre colonne, ou combinez plusieurs tris\n",
    "> - ajoutez plusieurs conditions avec les op√©rateurs logiques `&` (et), `|` (ou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56366188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40267154",
   "metadata": {},
   "source": [
    "> Pensez √† tester aussi Spark SQL dans cet exercice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f67979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f0b16c",
   "metadata": {},
   "source": [
    "## 4. Nettoyage des donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdd20a9",
   "metadata": {},
   "source": [
    "Dans Spark, nous pouvons appliquer des fonctions de transformation directement sur les colonnes d‚Äôun DataFrame de mani√®re distribu√©e.\n",
    "\n",
    "Dans l‚Äôexemple ci-dessous :\n",
    "- nous normalisons les noms de d√©partement\n",
    "- nous traitons les valeurs manquantes\n",
    "\n",
    "üí° Ces op√©rations sont aussi \"lazy\" : elles ne modifient pas les donn√©es tant qu‚Äôaucune action n‚Äôest ex√©cut√©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0494223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, trim, upper\n",
    "\n",
    "# Normaliser le nom du d√©partement: trim + upper\n",
    "df_sel = df\n",
    "df_clean = df_sel.withColumn(\"department_norm\", upper(trim(col(\"department\"))))\n",
    "\n",
    "# G√©rer les valeurs manquantes d'√¢ge et de salaire\n",
    "df_clean = df_clean.withColumn(\"age\", when(col(\"age\").isNull(), 0).otherwise(col(\"age\")))\n",
    "df_clean = df_clean.na.fill({\"salary\": 0.0})\n",
    "\n",
    "df_clean.select(\"employee_id\",\"name\",\"department\",\"department_norm\",\"age\",\"salary\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b29190",
   "metadata": {},
   "source": [
    "### 4.1. Cr√©er des colonnes calcul√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ddfdbf",
   "metadata": {},
   "source": [
    "La pr√©paration des donn√©es consiste aussi √† cr√©er de nouvelles colonnes d√©riv√©es √† partir des colonnes existantes.\\\n",
    "Dans Spark, cela se fait avec la m√©thode `withColumn()`, qui permet d‚Äôajouter une colonne calcul√©e sans modifier le DataFrame original.\n",
    "\n",
    "üëâ Si le nom de colonne existe d√©j√†, la colonne est remplac√©e (√©cras√©e) par la nouvelle version.\\\n",
    "üëâ Si le nom n‚Äôexiste pas, une nouvelle colonne est simplement ajout√©e au DataFrame, c'est le cas dans l'exemple suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec38bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Cat√©goriser les revenus\n",
    "df_feat = df_clean.withColumn(\n",
    "    \"salary_band\",\n",
    "    when(col(\"salary\") < 3000, \"LOW\").when(col(\"salary\") < 6000, \"MID\").otherwise(\"HIGH\")\n",
    ")\n",
    "\n",
    "df_feat.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8df1868",
   "metadata": {},
   "source": [
    "## 5. Regroupement et agr√©gation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a148e099",
   "metadata": {},
   "source": [
    "La phase d‚Äôagr√©gation permet de r√©sumer, regrouper et interpr√©ter les informations issues de grands volumes de donn√©es.\\\n",
    "Avec PySpark, ces op√©rations s‚Äôeffectuent facilement gr√¢ce aux fonctions de groupement (`groupBy`), de comptage (`count`), et d‚Äôagr√©gation (`agg`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d0372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre d'employ√©s par cat√©gorie de revenus\n",
    "df_feat\\\n",
    "    .groupBy(\"salary_band\")\\\n",
    "    .count()\\\n",
    "    .orderBy(col(\"count\").desc())\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d19134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round as sround\n",
    "\n",
    "# Analyse des salaires moyens par d√©partement \n",
    "dept_stats = df_feat\\\n",
    "                .groupBy(\"department_norm\")\\\n",
    "                .agg(sround(avg(\"salary\"),2).alias(\"avg_salary\"))\\\n",
    "                .orderBy(col(\"avg_salary\").desc())\n",
    "\n",
    "dept_stats.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71204139",
   "metadata": {},
   "source": [
    "### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68501691",
   "metadata": {},
   "source": [
    "> Comment Spark d√©coupe les transformations ci-dessus en **stages** ?  \n",
    "> Ouvrez l'onglet **SQL / DAG** dans l'UI Spark puis analysez les informations pr√©sent√©es.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9729eaa9",
   "metadata": {},
   "source": [
    "## 6. Sauvegarde de r√©sultats (CSV / Parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a9d300",
   "metadata": {},
   "source": [
    "Une fois les traitements r√©alis√©s nous pouvons sauvegarder les r√©sultats pour les r√©utiliser ou les partager.\\\n",
    "PySpark permet d‚Äô√©crire facilement un DataFrame dans diff√©rents formats, notamment CSV et Parquet (format binaire optimis√© pour le Big Data).\n",
    "\n",
    "- Le format CSV est universel mais peu efficace sur de gros volumes, car il ne conserve pas les types de donn√©es.\n",
    "- Le format Parquet, lui, conserve le sch√©ma, compresse les donn√©es et acc√©l√®re les lectures : il est donc privil√©gi√© dans les environnements distribu√©s.\n",
    "\n",
    "### üîé Format Parquet\n",
    "Pour aller plus loin concernant le format Parquet, voici quelques ressources :\n",
    "- üìò [Documentation officielle Apache Parquet](https://parquet.apache.org/docs/)  \n",
    "- üé• [Qu‚Äôest-ce qu'une base de donn√©es orient√©e colonne ?](https://www.youtube.com/watch?v=1MnvuNg33pA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf2ffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âcriture en CSV (dossier de sortie partitionn√©)\n",
    "dept_stats.write.mode(\"overwrite\").csv(\"./outputs/dept_stats_csv\")\n",
    "\n",
    "# √âcriture en Parquet (conserve le sch√©ma, plus efficace)\n",
    "dept_stats.write.mode(\"overwrite\").parquet(\"./outputs/dept_stats_parquet\")\n",
    "\n",
    "# Relecture du Parquet\n",
    "reloaded = spark.read.parquet(\"./outputs/dept_stats_parquet\")\n",
    "reloaded.printSchema()\n",
    "reloaded.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa168a38",
   "metadata": {},
   "source": [
    "## 7. RDD (Resilient Distributed Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ed253e",
   "metadata": {},
   "source": [
    "Jusqu‚Äô√† pr√©sent, vous avez manipul√© des **DataFrames** : une interface structur√©e, avec des colonnes nomm√©es, des types de donn√©es, et des op√©rations famili√®res (s√©lection, groupBy, agg, etc.).\\\n",
    "Les DataFrames sont tr√®s pratiques, car ils permettent d‚Äô√©crire du code lisible et optimis√© gr√¢ce au moteur **Catalyst** de Spark.\n",
    "\n",
    "Mais sous cette interface haut niveau se cache la brique fondamentale de Spark : le **RDD (Resilient Distributed Dataset)**.\\\n",
    "Comprendre ce concept vous permettra de mieux saisir comment Spark distribue, ex√©cute et reconstruit vos donn√©es √† grande √©chelle.\n",
    "\n",
    "Un RDD est une collection d‚Äôobjets distribu√©e √† travers le cluster Spark.\n",
    "- **Resilient** : il peut √™tre reconstruit automatiquement en cas de panne.\n",
    "- **Distributed** : les donn√©es sont d√©coup√©es en partitions r√©parties sur plusieurs n≈ìuds.\n",
    "- **Dataset** : c‚Äôest simplement un ensemble d‚Äô√©l√©ments (lignes, objets, etc.).\n",
    "\n",
    "Autrement dit, un RDD est une collection r√©partie sur plusieurs machines, que Spark sait manipuler en parall√®le."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e92668",
   "metadata": {},
   "source": [
    "### 7.1. Manipulation de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d483ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "# Exemple d'un RDD √† partir d‚Äôune liste locale\n",
    "# Spark cr√©e un RDD contenant ces 5 √©l√©ments.\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd7d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rien n‚Äôest ex√©cut√© tant qu‚Äôon ne d√©clenche pas une action\n",
    "display(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8cfe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut transformer les donn√©es\n",
    "rdd2 = rdd.map(lambda x: x * 2)\n",
    "display(rdd2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190466d3",
   "metadata": {},
   "source": [
    "#### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4800e6b8",
   "metadata": {},
   "source": [
    "> Soit la liste de produits :\n",
    "\n",
    "```bash\n",
    "ventes = [\n",
    "    (\"chaise\", 200),\n",
    "    (\"table\", 450),\n",
    "    (\"lampe\", 100),\n",
    "    (\"chaise\", 150),\n",
    "    (\"table\", 300),\n",
    "    (\"lampe\", 80)\n",
    "]\n",
    "```\n",
    "\n",
    "> - Cr√©ez un RDD √† partir de cette liste.\n",
    "> - R√©cup√©rez les produits dont le prix d√©passe 100‚Ç¨\n",
    "> - Calculez le CA total par produit (possible d'utiliser .reduceByKey())\n",
    "> - Triez par CA d√©croissant (possible d'utiliser .sortBy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c66290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f2d38c",
   "metadata": {},
   "source": [
    "### 7.2. Manipulation de fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e185b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark permet de lire et de manipuler des fichiers sous forme de RDD.\n",
    "employees_rdd = sc.textFile(\"./data/employees.csv\")\n",
    "employees_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d0cd4",
   "metadata": {},
   "source": [
    "#### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785a82dd",
   "metadata": {},
   "source": [
    "> Ne r√©cup√©rer que les employ√©s du d√©partement IT.\\\n",
    "> Indice : .map(), .filter(), .split()\n",
    "\n",
    "```bash\n",
    "Exemple :\n",
    "\n",
    "[['22', 'Alice Bernard', '33.0', 'IT', '6231.15', '10'],\n",
    " ['32', 'Hugo Girard', '58.0', 'IT', '8616.31', '20'],\n",
    " ['40', 'Emma Morel', '42.0', 'IT', '6115.84', '10'],\n",
    " ['72', 'Tom Petit', '41.0', 'IT', '5449.79', '7'],\n",
    " ['83', 'Ines Durand', '54.0', 'IT', '7555.4', '13']]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f5298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a966c69",
   "metadata": {},
   "source": [
    "## 8. Approfondir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d240ea",
   "metadata": {},
   "source": [
    "Ce notebook propose une **introduction pratique** √† Apache Spark et PySpark.  \n",
    "Il ne couvre pas toutes les subtilit√©s du framework, notamment celles li√©es √† l‚Äô**optimisation des performances**, la **gestion des clusters**, ou les **fonctionnalit√©s avanc√©es**, MLlib, ou Structured Streaming.  \n",
    "\n",
    "Pour compl√©ter votre apprentissage, il est **fortement recommand√© de poursuivre l‚Äôexploration** √† travers d'autres ressources.\\\n",
    "Il existe **de nombreuses ressources de qualit√© disponibles en ligne**, il suffit de chercher un peu pour trouver des explications, tutoriels et exemples adapt√©s √† votre niveau.\n",
    "\n",
    "L‚Äôobjectif de cet approfondissement est de comprendre Apache Spark dans son contexte Big Data, non pas comme un simple outil, mais comme une √©volution et une r√©ponse aux limites de Hadoop.\n",
    "Il s‚Äôagit d‚Äôidentifier les diff√©rences d‚Äôarchitecture, de performance et de paradigme de calcul, tout en comprenant comment ces deux technologies peuvent coexister et se compl√©ter.\n",
    "\n",
    "#### Axes d'approfondissement\n",
    "> Architecture de Spark\n",
    "- R√¥les du Driver, des Executors et du Cluster Manager\n",
    "- Traitement en m√©moire et ex√©cution des DAG (Directed Acyclic Graph)\n",
    "- Modes d‚Äôex√©cution : Local vs Cluster\n",
    "\n",
    "> Comparaison Hadoop / Spark\n",
    "- Stockage : HDFS et compatibilit√© avec Spark\n",
    "- Gestion des ressources : r√¥le de YARN\n",
    "- Mod√®le de calcul : batch vs in-memory\n",
    "\n",
    "> √âcosyst√®me et usages\n",
    "- Modules : Spark SQL, MLlib, Streaming, GraphX\n",
    "\n",
    "\n",
    "#### Ressources officielles\n",
    "- [Documentation officielle de Spark](https://spark.apache.org/docs/latest/) ‚Äî la source principale pour comprendre les modules (SQL, MLlib, Streaming, GraphX).  \n",
    "- [API PySpark sur spark.apache.org](https://spark.apache.org/docs/latest/api/python/) ‚Äî r√©f√©rence compl√®te des fonctions disponibles.  \n",
    "- [Apache Spark sur GitHub](https://github.com/apache/spark) ‚Äî pour explorer le code source et les √©volutions du projet.  \n",
    "\n",
    "#### Tutoriels et cours gratuits\n",
    "- [Spark Tutorial (DataCamp)](https://www.datacamp.com/tutorial/pyspark-tutorial-getting-started-with-pyspark) ‚Äî bonnes bases pour pratiquer avec PySpark.  \n",
    "- [YouTube ‚Äì ‚ÄúApache Spark Full Course‚Äù by Simplilearn](https://www.youtube.com/watch?v=_C8kWso4ne4) ‚Äî formation vid√©o compl√®te et vulgaris√©e.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
