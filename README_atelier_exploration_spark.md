# ğŸ¯ Objectif

Comprendre **Spark** dans son contexte **Big Data**, pas seulement comme un outil, mais comme une Ã©volution de **Hadoop**.

Identifier les diffÃ©rences dâ€™architecture, de performance et de paradigme de calcul, et comprendre comment les deux technologies **cohabitent**.

# ğŸ”¹ ThÃ¨me gÃ©nÃ©ral

Â« Comprendre Spark Ã  travers son lien avec Hadoop : continuitÃ© ou rupture ? Â»

# ğŸ”¹ Axes de recherche

1.	Architecture de Spark
- Driver, Executors, Cluster Manager
- Traitement en mÃ©moire
- DAG (Directed Acyclic Graph)
- Modes d'exÃ©cution (Driver) : Local vs Cluster
2.	Comparaison Hadoop / Spark
- Stockage (HDFS vs compatibilitÃ© Spark)
- Gestion des ressources (YARN)
- ModÃ¨le de calcul (batch vs in-memory)
3.	Ã‰cosystÃ¨me et usages
- Spark SQL, MLlib, Streaming, GraphX
- IntÃ©gration avec Hadoop et Hive