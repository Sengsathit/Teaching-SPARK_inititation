# 🎯 Objectif

Comprendre **Spark** dans son contexte **Big Data**, pas seulement comme un outil, mais comme une évolution de **Hadoop**.

Identifier les différences d’architecture, de performance et de paradigme de calcul, et comprendre comment les deux technologies **cohabitent**.

# 🔹 Thème général

« Comprendre Spark à travers son lien avec Hadoop : continuité ou rupture ? »

# 🔹 Axes de recherche

1.	Architecture de Spark
- Driver, Executors, Cluster Manager
- Traitement en mémoire
- DAG (Directed Acyclic Graph)
- Modes d'exécution (Driver) : Local vs Cluster
2.	Comparaison Hadoop / Spark
- Stockage (HDFS vs compatibilité Spark)
- Gestion des ressources (YARN)
- Modèle de calcul (batch vs in-memory)
3.	Écosystème et usages
- Spark SQL, MLlib, Streaming, GraphX
- Intégration avec Hadoop et Hive