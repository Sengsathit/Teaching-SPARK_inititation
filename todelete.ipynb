{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54f63150",
   "metadata": {},
   "source": [
    "![SPARK_PYSPARK](img/spark_pyspark.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a877b0ab",
   "metadata": {},
   "source": [
    "# üí´ Apache Spark : le moteur du Big Data moderne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4419d5",
   "metadata": {},
   "source": [
    "Le **Big Data** d√©signe des volumes de donn√©es trop grands pour √™tre trait√©s sur une seule machine.\n",
    "Historiquement, on utilisait **Hadoop MapReduce**, mais celui-ci lisait et √©crivait souvent sur disque ‚Üí lent.\n",
    "\n",
    "**Apache Spark** est n√© pour r√©pondre √† ce probl√®me.\n",
    "- **Traitement en m√©moire (RAM)** ‚Üí bien plus rapide\n",
    "- **API haut niveau** (SQL, DataFrame, MLlib, etc.)\n",
    "- **Multi-langages** : Scala, Java, Python (PySpark), R\n",
    "\n",
    "Spark repose sur une architecture driver / executors.\n",
    "- **Driver** : programme principal. G√®re le plan d‚Äôex√©cution et collecte les r√©sultats\n",
    "- **Cluster Manager** : alloue les ressources (CPU, RAM) aux t√¢ches Spark (YARN, Kubernetes, Standalone‚Ä¶)\n",
    "- **Executors** : ex√©cutent les t√¢ches sur les partitions de donn√©es.\n",
    "\n",
    "Spark distribue automatiquement les donn√©es et le calcul sur plusieurs machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a2174d",
   "metadata": {},
   "source": [
    "# ‚ú® PySpark : l‚Äôinterface Python de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93533013",
   "metadata": {},
   "source": [
    "**PySpark** est l‚Äôinterface de programmation en Python pour Apache Spark.\\\n",
    "Elle permet d‚Äôutiliser la puissance du moteur Spark tout en restant dans un environnement familier pour les d√©veloppeurs Python.\n",
    "\n",
    "Concr√®tement :\n",
    "- Le code √©crit en Python est traduit en instructions que le moteur Spark ex√©cute sur le cluster.\n",
    "- L‚Äôex√©cution r√©elle ne se fait pas dans le processus Python, mais dans la JVM de Spark (Java Virtual Machine).\n",
    "- Cette communication passe par l‚ÄôAPI Py4J, une passerelle entre Python et Java.\n",
    "\n",
    "Ainsi, PySpark combine :\n",
    "- la simplicit√© du Python (syntaxe, int√©gration avec pandas, notebooks, etc.)\n",
    "- la scalabilit√© de Spark, capable de traiter des t√©raoctets de donn√©es sur des centaines de noeuds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfea9e4",
   "metadata": {},
   "source": [
    "# ‚å®Ô∏è Hands-On"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd3bff",
   "metadata": {},
   "source": [
    "**Objectifs**  \n",
    "- Comprendre comment d√©marrer une `SparkSession` en local  \n",
    "- Manipuler des DataFrames PySpark (lecture, inspection, transformations)  \n",
    "- Comprendre `lazy evaluation`, `action`, `job / stage / task`  \n",
    "- Sauvegarder/relire des donn√©es (CSV, Parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615a17a9",
   "metadata": {},
   "source": [
    "## 1. D√©marrer Spark en local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa260e35",
   "metadata": {},
   "source": [
    "Avant de pouvoir manipuler des donn√©es avec PySpark, il faut cr√©er une session Spark.\\\n",
    "Cette session est le point d‚Äôentr√©e principal vers toutes les fonctionnalit√©s de Spark (lecture de donn√©es, SQL, transformations, machine learning, etc.).\n",
    "\n",
    "En pratique, on la cr√©e √† l‚Äôaide de la classe **SparkSession** qui permet de configurer et d‚Äôinitialiser le moteur Spark.\\\n",
    "Une fois cette cellule ex√©cut√©e, Spark d√©marre un contexte local et affiche des informations dans la console.\n",
    "C‚Äôest le signe que le driver est pr√™t √† ex√©cuter des t√¢ches et √† communiquer avec d‚Äô√©ventuels executors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7036a41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IntroSparkLocal\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# SparkSession.builder cr√©e un objet constructeur pour d√©finir les param√®tres de la session.\n",
    "# .appName(\"IntroSparkLocal\") d√©finit un nom d‚Äôapplication visible dans l‚Äôinterface Spark (utile pour identifier ton job).\n",
    "# .master(\"local[*]\") indique le mode d‚Äôex√©cution :\n",
    "#       - local signifie que le calcul se fera sur la machine locale.\n",
    "#       - [*] indique que Spark utilisera tous les c≈ìurs CPU disponibles.\n",
    "# .getOrCreate() cr√©e une nouvelle session Spark si elle n‚Äôexiste pas encore, ou r√©cup√®re celle d√©j√† active.\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c579803",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"./data/taxi.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55ce9a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'tpep_pickup_datetime',\n",
       " 'tpep_dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'RatecodeID',\n",
       " 'store_and_fwd_flag',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'congestion_surcharge',\n",
       " 'Airport_fee',\n",
       " 'cbd_congestion_fee']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
